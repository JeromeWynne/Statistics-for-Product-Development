% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% Example of the Memoir class, an alternative to the default LaTeX classes such as article and book, with many added features built into the class itself.

%\documentclass[12pt,a4paper]{memoir} % for a long document
\documentclass[11pt,a4paper,article]{memoir} % for a short document
\raggedbottom
\raggedright

\usepackage[utf8]{inputenc} % set input encoding to utf8
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{float}
\usepackage{parskip}
\usepackage{mathpazo}
\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%

%%% PAGE DIMENSIONS
% Set up the paper to be as close as possible to both A4 & letter:
\settypeblocksize{*}{13cm}{1.8} 
\setulmargins{*}{*}{1} % 50pt upper margins
\setlrmargins{*}{*}{1}% golden ratio again for left/right margins
\setheaderspaces{*}{*}{1.618}
\checkandfixthelayout 
% This is from memman.pdf

%%% ToC (table of contents) APPEARANCE
\maxtocdepth{subsection} % include subsections
\renewcommand*{\cftappendixname}{Appendix\space}

%%% HEADERS & FOOTERS
\pagestyle{plain} % try also: empty , plain , headings , ruled , Ruled , companion

%%% CHAPTERS
\chapterstyle{section} % try also: default , section , hangnum , companion , article, demo

%%% SECTIONS
\hangsecnum % hang the section numbers into the margin to match \chapterstyle{hangnum}
\maxsecnumdepth{subsection} % number subsections


\makeatletter
\renewcommand\tableofcontents{%
\null\hfill\textbf{\huge\contentsname}\hfill\null\par
  \vspace{14pt}
  \@mkboth{\MakeUppercase\contentsname}{\MakeUppercase\contentsname}%
  \@starttoc{toc}%
}

\renewcommand\contentsname{Table of Contents}

\makeatother

\renewcommand{\baselinestretch}{1.5} 

\graphicspath{{./images/}}


%% RUBRIC %%%%%%%%%%%%%%%%%%%%%%%%
% Style pointers
% > Write in the past tense
% > Passive
% > 'Each paragraph should contain a complete thought or argument'

% RUBRIC
% ---------------------------------------------------------------------------------
% Communication
% 	> Abstract
% 	> Clear statement of aims/objectives
% 	> Overall quality of report:
%		- Structure
%		- Appearance
%		- Use of English
%		- Grammar
%		- Typographical errors
%		- Nomenclature
%		- Symbols
%		- Acronyms
%		- References
% 	> Introduction to process/method
% 	> Background of industry/company context
% 	> Presentation and discussion of work and results
% 	> Conclusions and recommendations for further work
% 	> Quality of diagrams, tables
% 	> Sufficient references
% \/* "The report is well organised and well presented
% with excellent use of figures, tables and
% references. There are no spelling or grammatical
% errors. Complex information is communicated in a
% clear and logical manner." */
% \/*The purpose of the Industrial Critique report is to show that you can take an overview of a
% process, methodology or design exercise that you have personally had direct experience of, or
% been involved with, during your placement.*/
% STRUCTURE
% > Introduction
%		What DCA do, who their competitors are
%		What statistics is
%			Generic process overview
%				1. Design 2. Execute 3. Analyze 4. Present
%		Relevance of statistics to DCA
%		Outline of report structure
% > Overview of current methods
%		How DCA currently uses statistics
%			Process diagram
%			Analysis, experiment design, presentation/visualization
%			Evaluation
%		How DCA's competitors currently use statistics
%			Summary
%			Evaluation and comparison
% > Proposed alternatives
%		What methods DCA could use instead
%			Concepts
%				Analysis - Linear models (simple, multivariable, basis expansion, analysis of variance)
%				Experiment design - Blocking, factorial designs, and Taguchi
%				Data visualization - scatter and box plots, histograms
%			Implementation
%				Matlab, R, Minitab, Excel, Python
% > Conclusion

% Content
%	> Understanding of appropriate theory
%	> Understanding of process/method described
%	> Understanding of alternatives discussed
%	> Evaluation of alternatives discussed
%	> Critical discussion of work
%\/* The report is authoritative and the proposals/options
% described are suitable for implementation. An
% innovative approach is evident and there is a
% thorough awareness of the issues around
% implementation and of the economic feasibility of the
% proposals made. */
% B) How outcomes and results were checked and/or verified in DCA
% C) What went well and what did not go so well in the way statistics is done at DCA
% D) ALternative processes methods which could have been used toghther with the procs and cons of each of each of these
% E) Recommendations for improvements that could be made to the process used, giving details of the implications of these
% The focus of this critique is not on the detail of what you did or achieved but on your wider and
% deeper understanding of how the work was carried out, why it was carried out in the way it was,
% and on the alternative approaches that could have been taken. Technical detail is not a
% fundamental requirement of the critique (though should be included at an appropriate level) – it is
% the evaluation of the process/methodology that is important.

% Consider both operational and financial implications of the alternatives recommended.

% The ICR requires that you detail a critique of a process, methodology or design exercise. You are
% expected to evaluate the pros and cons of alternative approaches which may not be is use in your
% placement company and will therefore require you to carry out some research into what your
% company does and what is done elsewhere. The report must also make some recommendations,
% which may also propose further evaluation or work to implement and changes you are proposing.
% You may also need to support your conclusions and recommendations with some quantitative
% assessments of time, cost, efficiency improvements etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%% TITLE PAGE
\newlength\drop
\makeatletter
\newcommand*\titleM{\begingroup% Misericords, T&H p 153
\setlength\drop{0.1\textheight}
\centering
\vspace*{\drop}
{\Huge\bfseries Statistics \vspace{14pt} for \vspace{14pt}  Product Development}\\ 
\vspace{1in}
{Jerome Wynne}\\[\baselineskip]
{\scshape University of Bristol}\par
\vspace{0.61in}

%%% ABSTRACT %%
% – Set the scene (background)
% – State why the topic of your ICR is important (within the
% context of the background)
% – State why/how what was done is different or unique (or
% why it was carried out)
% – Briefly summarise results, conclusions and
% recommendations
{\bfseries Abstract}\\[\baselineskip]
{Analyzing a product's performance during development is essential to making informed design decisions, yet many engineers are uncomfortable using statistics. This shouldn't be the case: statistical tools can be invaluable for recognizing patterns in experimental data, and therefore offer a means of improving the quality and consistency of design decisions. Here, DCA's current use of statistics is evaluated relative to the medical industry as a whole. Experiment design, analysis, and presentation tools are suggested that would enhance DCA's testing process. These tools are evaluated against the realities of DCA's work by considering how they might be implemented in DCA's experimental procedures.}
\vfill

{\scshape \@date}\par
\endgroup}
\makeatother

% MULTILINE COMMENTS
\long\def\/*#1*/{}

%% START OF DOCUMENT
\begin{document}

\begin{titlingpage}
\titleM
\end{titlingpage}

\tableofcontents* % the asterisk means that the contents itself isn't put into the ToC
\firmlists

% List of tables/figures
\newpage
\listoftables
\listoffigures

% Notation
\newpage
\chapter*{Notation \& Glossary}
\begin{tabular} {p{2.7cm}p{10cm}}
\textbf{Attribute} & A measurable property of a \textit{unit}. \\[0.5cm]
\textbf{Block} & A set of \textit{units} thought to share some common \textit{attribute} that influences their \textit{response}. \\[0.5cm]
\textbf{Event} & A set of \textit{outcomes}.\\[0.5cm]
\textbf{Experiment$^{1}$} & The controlled collection of data. \\[0.5cm]
\textbf{Experiment$^{2}$} & Physically realizing an outcome of the system under study. \\[0.5cm]
\textbf{Factors} & \textit{Treatments} that are discrete. For example, lubricated/unlubricated.\\[0.5cm]
\textbf{Outcome} & A possible result of a \textit{trial}. \\[0.5cm]
\textbf{Probability} & A method for quantifying uncertainty, or a value representing the uncertainty of an event. \\[0.5cm]
\textbf{Response} & The measured performance of a \textit{unit}. \\[0.5cm]
\textbf{Treatment} & A modification applied to a \textit{unit}.\\[0.5cm]
\textbf{Unit} & A single test specimen - in the context of product testing, this is likely to be a prototype build of the product.
\end{tabular}

\newpage
\chapter*{\large Acknowledgements}
\vspace*{-\baselineskip}
Beyonc\'{e}, J.D.Sallinger, and Santa Claus. \\
This report would not have been possible without the encouragement of Paul Harper and technical supervision of Sophie Sladen. More broadly, I am grateful to DCA Design International for providing me a place to work on these ideas and develop professionally. Thank you to DCA's engineers - especially Will Marsh, Matthew Jones,   and Matthew Edwards - for setting the bar so high and for helping me to improve as an engineer.
\chapter*{\large Declaration}
\vspace*{-\baselineskip}
I confirm that the work presented here is wholly my own and has been generated as a result of my own thought and study. Where I have consulted the work of others it is mentioned, and where my work was part of a group effort my contribution is made clear. Where the work of another is quoted, the source is given.



%% INTRODUCTION %%
\newpage
\chapter{Introduction}
% A) Why statistics was used in DCA
DCA Design International is a 150-person product design consultancy based in Warwick. Their work is oriented towards the mechanical design of medical and consumer products. Much of what they develop is hand-held items such as insulin injector pens or deoderant cans. DCA's competitors are [DCA's COMPETITORS AND THEIR CAPABILITIES].
\par
DCA employs about sixty mechanical engineers. Each of these act as general-purpose technical consultants and as experts in a particular engineering subdiscipline. DCA's substantial investment in engineering distinguishes it from other product design consultancies, many of which do not have the capabilities to handle a product's technical development [REFERENCE]. This investment is manifest not only in the number of DCA's mechanical engineers, but also their ownership of four test labs, each of which contains a plethora of engineering instruments and machines.
\par
This equipment is regularly used by DCA's engineers to generate experimental data. The way in which this data is collected, analyzed, and presented is the focus of this report. DCA conducts these data-oriented activities, which together can be referred to as ``statistics'', in non-engineering departments such as human factors or operations research, but here only their use of them in the context of lab investigations is evaluated.
\par
To reiterate, ``statistics'' in this report refers to the systematic collection, analysis, and presentation of data. Some statistical methods are justified via probability theory, which is a logically consistent way to describe uncertainty. Other techniques are pragmatic, such as visualizing data in a way that is not misleading, or appropriately defining an experiment's scope. The defining characteristic of statistical methods is that they seek to make the best use of the information and resources available.
\par
Statistics is most useful if it's applied before, during, and after an experiment. This is because it encourages forethought about what exactly needs to be known, what factors might create uncertainty during analysis, and how these factors can be mitigated within the experiment itself. Statistical tools have been designed around a process consisting of four steps:
\begin{itemize}
\item Design an experiment to capture the desired information.
\item Execute said experiment in a controlled fashion.
\item Analyze the resulting dataset.
\item Present and document the analyses' results.
\end{itemize}
``Experiment'' refers exclusively to the data generation procedure of the second step, whereas ``statistical process'' denotes the combination of all four steps.
The statistical process can be applied at various levels: for example, the principles within can be applied to a series of experiements.
\par
In summary, this report analyzes how DCA currently uses and could use statistics in its lab investigations. Section X1 explains their investigatory framework and how statistics is currently applied within it. In Section X2 approach is then compared to the SOMETHING industry as a whole. This is followed by Section X3, in which statistical methods are suggested that DCA's engineers and clients may find relevant. These methods are introduced conceptually, then tools (i.e. software and tangibles) for implementing them showcased. The report concludes with an evaluation of how actionable these suggestions are, and a suggestions for further work.

\newpage



%% DCA'S USE OF STATISTICS IN LAB INVESTIGATIONS %%
\chapter {Overview of DCA's Use of Statistics in Lab Investigations}
This section contextualizes DCA's lab investigations by explaining what classes of products require lab work, when this work is undertaken, and what equipment is used when it is. It outlines the stucture of such an investigation before explaining what statistical methods are currently being applied by DCA's engineers. These methods are categorized according to their relevance to the steps mentioned in the preceding Section, and are ranked according to their benefits and shortcomings. The Section concludes with a summary of how statistical methods are applied in DCA and an overview of the collective strengths and weaknesses of the suite of tools that are currently being used.
\section{The Structure of a Lab Investigation in DCA}
% What departments in DCA test their products?
Lab investigations are used to reduce uncertainty about the behaviour of a product or process. They consist of a sequence of experiments aimed at understanding a particular aspect of a product's or process's behaviour. By collecting data, DCA's engineers aim to become better informed. This allows them to make design decisions that are both more effective and justified than they might be otherwise. Within DCA, lab investigations are usually conducted in the development of a medical device. They may be initiated by a client, or by an internal engineering team on behalf of a client. Their timeframe can range from less than a week to several months. 
 \par
 Typically, an investigation focuses on a particular product parameter, such as the volume of fluid dispensed by an injector, or the propensity of a inhaler to fail upon being dropped. Occasionally engineers working on fast moving consumer goods (such as toothbrushes or lotion bottles) will run one-off tests to compare design variations or verify performance relative to some baseline. In general however, the timeframes and functional requirements of such products limit the relevance of extensive experimental investigations. Subsequently most lab work is conducted by engineers on medical projects.
\par
% When do they test them and why?
Lab work may occur at any point during a medical product's development, which will typically last between three and seven years. As can be seen in Figure \ref{fig:time_of_tests} however, most experiments are run in the late stages of the design process. This is when the product's design has been largely locked down, which has various implications: resolving minor performance issues becomes a worthwhile pursuit, exploratory tests for future product variants become a possibility, and rehearsal for fast-approaching regulatory tests becomes essential.
\begin{figure}
\label{fig:time_of_tests}
\end{figure}
\par
% How do they test them?
DCA's engineers have access to lab-grade equipment capable of measuring and controlling a variety of physical properties. These include axial and torsional testing machines, enclosed environment chambers, coordinate measuring machines, mass balances, high-speed cameras, and so on. Investigations commonly revolve around a particular experimental set-up, however ancillary experiments are often designed to provide supplementary information. This report attempts to be data-agnostic in its recommendations of analytical techniques, insofar as experimental observations can be coded in a numerical format.
\par
DCA's engineers run experiments to either:
\begin{itemize}
	\item Investigate a product's behaviour
	\item Verify that a product will meet regulatory standards
\end{itemize}
Investigative tests seek to understand and measure the influence of design alterations. These alterations could be prospective features in the final product, or they may be modifications to exaggerate particular behaviours of the current design. An example of the latter would be to machine the components to their tolerance extremes in order to check whether their mechanism still functions correctly. They are especially useful during the late phases of a product's development when minor improvements in performance are sought through incremental design alterations. They are also used to discriminate between contending conceptual designs in early to mid-phase work.
\par
Verification tests are run to ensure a product meets the standards enforced by the authorities regulating a product's destination market. Common standards, such as those produced by the International Standards Organization (ISO), are typically used by regulatory bodies - such as the Food and Drug Administration in the United States and the Medicines and Healthcare products Regulatory Agency in the UK - to inform a given market's regulatory standards. DCA's products must meet a market's regulatory standards, but do not necessarily have to meet the general standard (i.e. the relevant ISO standard).  Consequently, the validation tests DCA run are designed to imitate the tests stipulated by the regulatory standards.
\par
% What does a testing sequence look like?
The structure of an investigation itself is outlined in Figure \ref{fig:investigation_diagram}. 
\begin{figure}
\ref{fig:investigation_diagram}
\caption{Investigation diagram.}
\end{figure}

% Identify need for data
%	> Define investigation objective
%	> Acquire relevant equipment and expertise
%	> Identify routes of enquiry
%	> Identify relevant response and predictor variables

% Experiment design
%	> Replicable, reliable, valid
%	> Blocking
%	> Randomization
%	> Orthogonality
%	> Factorial experiments

% Experiment execution

% Analysis

% Presentation

% Conclusion or Further experimentation



%%
\section{Experiment Design}
Good experimental design makes it vastly easier to unambiguously interpret the results of an experiment. The essentials of good design are
\begin{itemize}
\item Controlling variation of units
\item Structured application of treatments
\item Designing according to an objective
\end{itemize}
Topics covered:
\begin{itemize}
\item Block design
\item Factorial design
\item Sample sizing
\end{itemize}

Experiment design in DCA is on an ad-hoc basis and will apply simple but effective techniques such as blocking or factorial structures. DCA's engineers are aware of the important factors to control during an experiment, and may in some cases iterate upon an experiment's set-up until they are satisfied that the data is not being corrupted by nuisance factors.
Certain documents must be kept for all experiments run - this means that raw data files, scans of handwritten observations, a table of the components used, and an Excel report summarizing the experiment's results must be produced and stored.

%%
\section{Experiment Control}

%%
\section{Analysis}

 Some experiments have specific protocols and prescribed analysis procedures. Other, more novel, experiments do not have a documented procedure and will usually have a spontaneously formatted report, since there is no general template.  To the best of my knowledge, there are no internal guidelines regarding how data should be visualized, or how analytical results should be verified. Broadly speaking, an analysis will typically consist of
 \begin{itemize}
 	\item Line charts of raw data
 	\item Summary statistics by group or unit, sometimes charted in a scatter plot
 	\item Summary of derived results from graphical inspection of the data
 \end{itemize}
 For the better-documented tests, hypothesis tests may also be run.
\par



\newpage
%% MEDICAL INDUSTRY'S USE OF STATISTICS %%
\chapter{Overview of the Medical Industry's Use of Statistics}\label{industry_context} %% CONTEXT %%

\section{Experiment Design}

\section{Analysis}
\subsection{Exploratory Analysis}
\subsection{Descriptive Analysis}
\subsection{Inferential Analysis}

\section{Presentation \& Visualization}

\section{Summary}

\newpage


%% PROPOSED METHODS %%
\chapter{Suggested Methods}
%%
\section{Design of Experiments}
\subsection{Blocking}
\subsection{Factorial designs}
\subsection{Taguchi}
\subsection{Strategies for handling limited sample sizes}

%%
\section{Analysis}
\subsection{Sumary Statistics}
\subsection{Regression Models}
Regression models are a flexible tool that allow us to
\begin{itemize}
\item Estimate the effects of discrete and continuous factors on a unit's response - even if they interact or have a nonlinear influence.
\item Identify differences between units that aren't immediately apparent from raw data
\item Measure how well our understanding of a product lines up with its reality
\end{itemize}
\subsection{Bayesian Inference}
\subsection{Markov Chain Simulation}

%%
\section{Presentation \& Visualization}
\subsection{The Psychology of a Plot}
\subsection{Scatter plots}
\subsection{Histograms}
\subsection{Box plots}
\subsection{Separation Plots}
http://mdwardlab.com/sites/default/files/GreenhillWardSacks.pdf

%%
\section{Software}
\begin{itemize}
\item Excel
\item Matlab
\item R
\item Python
\item Minitab
\end{itemize}

\newpage


%% CONCLUSIONS AND RECOMMENDATIONS %%
\chapter{Conclusions \& Recommendations}

\newpage
\appendix
\chapter{}

% Ronald Fisher, The Design of Experiments
% Attacking the interpretation or data collection method
% `an experimenter... wishes to safeguard his results, ... from ignroant criticism by different sorts of superior persons'
% Criticizes Bayes' theorem: ` Advocates of inverse probability seem forced to regard mathematical probability not as an
%					objective quantity measured by observable frequencies, but as measureent merely
%					psychological tendencies'
% `Experimental observations are only experience planned in advance, and designed to form a secure basis of new knowledge'
% Refs: 	- An essay towards solving a problem in the doctrine of chances, T. Bayes (1763)

% R. Mead, Design of Experiments
% Introduction
% ---------------
% - Degrees of freedom - count of the number of independent comparisons that can be made
% - The total information in an experiment involving N experiental units may be represented by the total variation based on N -1 df
%		> Variation is divided into three components:
%			- Treatment component T, sum of df corresponding to the questions to be asked
%			- Blocking component B, representing all environment effects that are to be included in the fitted model.
%			  (b - 1)df for b blocks.
%			-  Error component E, used to estimate the error's variance (i.e. estimate the standard errors)
%	The resource equation:	T + B + E = N - 1
%	To obtain a good estimate of error, need at least 10 df (preferably 15)
%	- Calculable by examining the 5% point of the t-distribution, and using enough df that increasing the error df
%	  makes little difference to the significance value.
%
% One final comment on this question of efficient use of resources. It is, of course, possible
% to keep the value of E in the region of 10–20 by doing several small experiments, rather than
% one rather larger experiment in which the total number of experimental units is much less
% than the sum of the units in the separate small experiments. This is inefficient because of the
% need to estimate σ2 for each separate experiment. Experimenters should identify clearly the
% questions they wish the experiment to cover, and they should also consider carefully if they
% are asking enough questions to use the experimental resources efficiently.

% Randomized complete block design: Essentially each group of ‘similar’ units
%					        should include roughly equal numbers of units for each treatment. 
%					        This control is referred to as blocking.
%						- `Similar' units -> e.g. same spring orientation, batch numbers, cavity numbers
%					       Each block contains a single replicate of each of the treatments.
%					       Key idea is to make groups subject to similar sources of variation.
%					      A `block' is a group of similar units.
% Completely randomized design: Randomly allocate treatments to units.
%

% Analysis of variance
% - Provides a subdivision of the total variation between the exerimental units into separate components, each component
%	representing a different source of variation, so that the relative importance of the different sources can be assessed.
% - Gives and estimate of the underlying variation between units, providing a basis for inferences about the applied
%	treatments.
% The basic philosophy of analysis of variance is to assume that each unit has an inherent
% response, possibly related to the position of the unit within the experiment, which is modified
% by the effect of the particular treatment applied to the unit.
% Response in terms of components y_{ij} = \mu + b_i + t_j _ \epsilon_{ij}
% \mu is the average of the whole set of experimental units, b_i is the term representing the average deviation
% from \mu of the units in block i, t_j is the term representing the average deviation from the \mu of the units given treatment j,
% \epsilon_{ij}  represents the deviation of a particular unit from \mu + b_i.
% sum over t_j must equal zero, as must the sum over b_i.
% Assumptions: - Individual unit variation is not affected by treatment applied to the unit.
%		      - The effect of each treatment is additive - the chosen block does not affect the efficacy of the treatment.
% The analysis of variance of data from an experimental design is simply the division of the
% total variation between the n observations into sensibly distinguishable components, and the
% subsequent interpretation of the relative size of the components
% Applying this principle to the randomized block design, we get
% \[
%	total SS = block SS + treatment SS + error SS
% \]
% Standard error of the difference between treatments
% In addition, it is natural, when making comparisons, to concentrate on
% those comparisons which look larger, which will be between the treatments giving the more
% extreme results and which are then subject to a selection bias.n addition, it is natural, when making comparisons, to concentrate on
% those comparisons which look larger, which will be between the treatments giving the more
% extreme results and which are then subject to a selection bias.

% Multiple blocking and recording of nuisance variables
\end{document}